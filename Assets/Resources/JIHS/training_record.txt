mlagents-learn config/agent_config.yaml --run-id=test1 --env="Build/Final Project Testing V2" --num-envs=2
mlagents-learn config/agent_config.yaml --run-id=test10 --env="Build2/Final Project Testing V2" --num-envs=2    

gail in yaml: beat recorded version

demonstration recorded on prefab to record the performance for gail

behaviour type : heuristic only in prefab

mean reward ~1 for gail recording

when training : record off, behaviour type to default

things about GAIL and behavioural cloning:
https://github.com/yosider/ml-agents-1/blob/master/docs/Training-Imitation-Learning.md
https://github.com/yosider/ml-agents-1/blob/master/config/gail_config.yaml
https://github.com/gzrjzcx/ML-agents/blob/master/docs/Training-Imitation-Learning.md

test6 starts using GAIL (test3)
test7 uses same GAIL (test3) and behavioural cloning (test3)
test8 - GAIL + BC (both test3) + 2 envs, adjusted values since I noticed they weren't really bringing the balls back home (still -ve rewards...)
Agent.onnx is based on test8 - honestly not too bad? sometimes there's no movement whatsoever, doesn't bring balls back to base

Build2 uses protect and offense incentives
test9 - GAIL + BC (both test3) + 2 envs (ran into warning: Your environment contains multiple teams, but PPOTrainer doesn't support adversarial games. Enable self-play to train adversarial games.)
test10 - GAIL + BC (both test3) + 2 envs (offense-collecting-targets = 1.5, test9 had 2.5, redefined protect to make sure there's no targets that aren't in a base)
Agent1.onnx is based on test9 - worse than Agent.onnx, wasn't trained too long